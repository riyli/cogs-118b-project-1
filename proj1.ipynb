{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed910f",
   "metadata": {
    "id": "77ed910f"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a09a75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38a09a75",
    "outputId": "d5d5917a-4e14-4270-82f5-65655cdd8ea3"
   },
   "outputs": [],
   "source": [
    "(train_set, train_label), (test_set, test_label) = mnist.load_data()\n",
    "train_set, train_label = train_set[:20000], train_label[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928733ac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "928733ac",
    "outputId": "2354d61a-fc8f-4774-ce63-96446eca6fda"
   },
   "outputs": [],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da43a200",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "da43a200",
    "outputId": "ee752d6d-59cb-49bd-ba9c-a6d5b1e95a44"
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_set[5][:,:], cmap = plt.cm.binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf2465f",
   "metadata": {
    "id": "3bf2465f"
   },
   "outputs": [],
   "source": [
    "#Reshape the images as a 784d vector\n",
    "train_set = train_set.reshape(20000,-1)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778ae14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9778ae14",
    "outputId": "18ba8db7-83df-439f-e43d-c99095651d74"
   },
   "outputs": [],
   "source": [
    "#Binarising the data for use in Bernoulli \n",
    "train_set[train_set < 0.5] = 0\n",
    "train_set[train_set > 0.5] = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80ce5b",
   "metadata": {
    "id": "cf80ce5b"
   },
   "source": [
    "# Section 1. Implementing a Multi Dimensional Bernoulli random variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec387d",
   "metadata": {
    "id": "98ec387d"
   },
   "source": [
    "Approach here is to treat each image as a vector of dimension 784. Since we normalised the image, each pixel in the image will be in the range [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a325e825",
   "metadata": {
    "id": "a325e825"
   },
   "source": [
    "Assume that for a given image vector $X_i$, it has pixels $$X_i = [x_{i1}, x_{i2}, x_{i3}, .... , x_{id}]$$Our bernoulli random variable follows the distribution $$p(x_j | \\theta_j) = \\theta_j^{x_j} (1 - \\theta_j)^{1 - x_j}$$So for each pixel belonging to $X_i$, it follows the above distribution. The main assumption again that we follow is that the pixels in a given image are independent of each other. So by the rule of independence, we have $$\\begin{equation}\n",
    "p(X_i | \\theta) = \\Pi_{j=1}^d p(x_{ij} | \\theta_j)\n",
    "\\end{equation}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3084bda5",
   "metadata": {
    "id": "3084bda5"
   },
   "source": [
    "Now that we know the probability distribution for a single image, we compute the total data likelihood which is the product of the above distribution for each image i.e. $$\\begin{equation} p(D| \\theta) = \\Pi_{i = 1}^N p(X_i | \\theta) \\end{equation}$$\n",
    "So we have $$\\begin{equation} p(D | \\theta) = \\Pi_{i = 1}^N \\Pi_{j = 1}^d \\theta^{x_{ij}} (1 - \\theta)^{1 - x_{ij}} \\end{equation} $$\n",
    "\n",
    "Now you can take the log likelihood of the data and differentiate w.r.t $\\theta$ to get the MLE estimate of the bernoulli random variable for the mnist dataset. After differentiation we get $$\\begin{equation} \\theta_j^{MLE} = \\frac{\\Sigma_{i = 1}^N x_{ij}}{N} \\end{equation}$$ which intuitively means that the $j^{th}$ component of $\\theta^{MLE}$ is simply the sum of the $j^{th}$ pixel value across all images. \n",
    "\n",
    "Therefore this says that if we want to say get the Bernoulli Model for the MNIST, we want the $\\theta$ to be the mean of each pixel across all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda9b97b",
   "metadata": {
    "id": "bda9b97b"
   },
   "outputs": [],
   "source": [
    "# define theta_MLE method, get MLE estimate of theta for mnist\n",
    "\n",
    "def get_theta_MLE(X: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    use the equation thetamle_j = sum(x_ij) / N to find MLE estimate of theta\n",
    "\n",
    "    params:\n",
    "        X: 2d numpy array of size N*j, of 0 or 1 representing an image\n",
    "\n",
    "    returns: \n",
    "        theta_ml, 1d numpy array of size j\n",
    "    '''\n",
    "\n",
    "    n, d = X.shape # get dimensions of input array\n",
    "    theta_MLE = np.zeros(d) # initialize output array size\n",
    "\n",
    "    for j in range(d): # iterate up to j, use equation\n",
    "        theta_MLE[j] = sum(X[i][j] for i in range(n)) / n \n",
    "\n",
    "    return theta_MLE \n",
    "\n",
    "theta_MLE = get_theta_MLE(train_set) # outputs 1x784 array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407463d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "407463d4",
    "outputId": "237acea7-1219-4377-f300-fd4962f9e9ae"
   },
   "outputs": [],
   "source": [
    "#Visualisation of the MLE estimate\n",
    "theta_MLE = theta_MLE.reshape(28,28)*255\n",
    "plt.imshow(theta_MLE, cmap = plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccdd168",
   "metadata": {
    "id": "5ccdd168"
   },
   "source": [
    "# Section 2. Implementing KMeans on MNIST to cluster data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6afc60f",
   "metadata": {
    "id": "d6afc60f"
   },
   "source": [
    "So we have studied KMeans as a very intuitive approach to clustering data. Suppose we want to categorize each image into a specific cluster. How do we do that ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eae879",
   "metadata": {
    "id": "68eae879"
   },
   "outputs": [],
   "source": [
    "def calcSqDistances(X, Kmus):\n",
    "\n",
    "    '''\n",
    "    assign each data vector to closest mu vector as per Bishop (9.2)\n",
    "    do this by first calculating a squared distance matrix where the n,k entry\n",
    "    contains the squared distance from the nth data vector to the kth mu vector\n",
    "    '''\n",
    "\n",
    "    # get dimensions\n",
    "    n, d = X.shape\n",
    "    k = len(Kmus)\n",
    "\n",
    "    # initialize array of all zeroes\n",
    "    distances = np.zeros((n, k))\n",
    "\n",
    "    # iterate through X and Kmus, populate array\n",
    "    for i, data in enumerate(X):\n",
    "        for j, mu in enumerate(Kmus):\n",
    "            distances[i][j] = np.linalg.norm(data - mu)\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb016048",
   "metadata": {
    "id": "fb016048"
   },
   "outputs": [],
   "source": [
    "def determineRnk(sqDmat):\n",
    "    '''\n",
    "    given the matrix of squared distances, determine the closest cluster\n",
    "    center for each data vector\n",
    "    '''\n",
    "\n",
    "    # get dimensions\n",
    "    len_m, len_n = len(sqDmat), len(sqDmat[0])\n",
    "\n",
    "    # initialize array of all zeroes, get minima of axis\n",
    "    responsibility = np.zeros((len_m, len_n))\n",
    "    min_axis = np.argmin(sqDmat.T, axis = 0)\n",
    "\n",
    "    for i, j in enumerate(min_axis):\n",
    "        responsibility[i][j] = 1\n",
    "    \n",
    "    return responsibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaf8800",
   "metadata": {
    "id": "9eaf8800"
   },
   "outputs": [],
   "source": [
    "\n",
    "def recalcMus(X, Rnk):\n",
    "    '''\n",
    "    recalculate mu values based on cluster assignments as per Bishop (9.4)\n",
    "    '''\n",
    "\n",
    "    # get dimensions\n",
    "    n, d = X.shape\n",
    "    n, k = Rnk.shape\n",
    "\n",
    "    # initialize array of all zeroes\n",
    "    k_mus = np.zeros((n, d))\n",
    "    rnk_sum = [ sum(Rnk[i]) for i in range(n) ] \n",
    "\n",
    "    # populate k_mus\n",
    "    for i in range(k):\n",
    "        for j in range(n):\n",
    "            k_mus[j][i] = Rnk[j][i] * X[j][i] / rnk_sum[j]\n",
    "\n",
    "    return k_mus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d1b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCurrent(X, Rnk, Kmus):\n",
    "    N, D = X.shape\n",
    "    K = Kmus.shape[0]\n",
    "\n",
    "    InitColorMat = np.array([[1, 0, 0],\n",
    "                             [0, 1, 0],\n",
    "                             [0, 0, 1],\n",
    "                             [0, 0, 0],\n",
    "                             [1, 1, 0],\n",
    "                             [1, 0, 1],\n",
    "                             [0, 1, 1]])\n",
    "\n",
    "    KColorMat = InitColorMat[0:K,:]\n",
    "\n",
    "    colorVec = np.dot(Rnk, KColorMat)\n",
    "    muColorVec = np.dot(np.eye(K), KColorMat)\n",
    "    plt.scatter(X[:,0], X[:,1], c=colorVec)\n",
    "\n",
    "    plt.scatter(Kmus[:,0], Kmus[:,1], s=200, c=muColorVec, marker='d')\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f0e2ee",
   "metadata": {
    "id": "76f0e2ee"
   },
   "outputs": [],
   "source": [
    "def runKMeans(K):\n",
    "    #load data file specified by fileString from Bishop book\n",
    "    # X = np.loadtxt(fileString, dtype='float')\n",
    "    X = train_set\n",
    "\n",
    "    #determine and store data set information\n",
    "    N, D = X.shape\n",
    "\n",
    "    #allocate space for the K mu vectors\n",
    "    Kmus = np.zeros((K, D))\n",
    "\n",
    "    #initialize cluster centers by randomly picking points from the data\n",
    "    rand_inds = np.random.permutation(N)\n",
    "    Kmus = X[rand_inds[0:K],:]\n",
    "\n",
    "    #specify the maximum number of iterations to allow\n",
    "    maxiters = 1000\n",
    "\n",
    "    for iter in range(maxiters):\n",
    "        #assign each data vector to closest mu vector as per Bishop (9.2)\n",
    "        #do this by first calculating a squared distance matrix where the n,k entry\n",
    "        #contains the squared distance from the nth data vector to the kth mu vector\n",
    "\n",
    "        #sqDmat will be an N-by-K matrix with the n,k entry as specfied above\n",
    "        sqDmat = calcSqDistances(X, Kmus)\n",
    "\n",
    "        #given the matrix of squared distances, determine the closest cluster\n",
    "        #center for each data vector\n",
    "\n",
    "        #R is the \"responsibility\" matrix\n",
    "        #R will be an N-by-K matrix of binary values whose n,k entry is set as\n",
    "        #per Bishop (9.2)\n",
    "        #Specifically, the n,k entry is 1 if point n is closest to cluster k,\n",
    "        #and is 0 otherwise\n",
    "        Rnk = determineRnk(sqDmat)\n",
    "\n",
    "        KmusOld = Kmus\n",
    "        plotCurrent(X, Rnk, Kmus)\n",
    "        time.sleep(1)\n",
    "\n",
    "        #recalculate mu values based on cluster assignments as per Bishop (9.4)\n",
    "        Kmus = recalcMus(X, Rnk)\n",
    "\n",
    "        #check to see if the cluster centers have converged.  If so, break.\n",
    "        if np.sum(np.abs(KmusOld.reshape((-1, 1)) - Kmus.reshape((-1, 1)))) < 1e-6:\n",
    "            print(iter)\n",
    "            break\n",
    "\n",
    "    plotCurrent(X, Rnk, Kmus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4775eca3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4775eca3",
    "outputId": "f9ee4042-e5e5-484e-ffdd-4599773dbfcd"
   },
   "outputs": [],
   "source": [
    "Kmus_10 = runKMeans(10)\n",
    "Kmus_20 = runKMeans(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1249d8c3",
   "metadata": {
    "id": "1249d8c3"
   },
   "source": [
    "# We can see which cluster roughly represents which digit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f655d04b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "f655d04b",
    "outputId": "a2584e24-9679-45c5-d387-70ed896aabf2"
   },
   "outputs": [],
   "source": [
    "def get_cluster_plot(Kmus): \n",
    "    '''\n",
    "    : get a plot of what different clusters represent\n",
    "    : takes as input the cluster location matrix\n",
    "    '''\n",
    "    fig,axs = plt.subplots(2,5, figsize = (15,9))\n",
    "    for i in range(len(Kmus)):\n",
    "        cluster = Kmus[i].reshape(28,-1) * 255\n",
    "        axs[i//5][i%5].imshow(cluster, cmap = plt.cm.binary)\n",
    "        axs[i//5][i%5].set_title(f'Cluster {i + 1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76051c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cluster_plot(Kmus_10) #Get cluster representation for K = 10 clusters\n",
    "get_cluster_plot(Kmus_20) #Get cluster representation for K = 20 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecRouqfTbByb",
   "metadata": {
    "id": "ecRouqfTbByb"
   },
   "source": [
    "# 3. Implementation of a Mixture of Bernoulli Distributions\n",
    "\n",
    "We have seen the performance of a single Bernoulli Distribution on the dataset. We saw it was not very good. On the other hand, using KMeans, we saw that same digits are having different cluster centers based on the writing style. Ex : In the image above, we can see that 0 is associated with clusters 5,7,13 and so on. So we hypothesise that a single digit cannot be entirely sampled from a single distribution and hence a mixture model maybe able to better explain the datas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7GTvHa9hbDWo",
   "metadata": {
    "id": "7GTvHa9hbDWo"
   },
   "source": [
    "Let us assume that our model is actually a Mixture of Multivariate Bernoulli random variable. For now, let us assume that the model consists of 10 multivariate bernoulli distributions. In this case, given a vector of pixel values, we don't know which cluster it belongs to. So let $z = \\{1,2,3,4,5,...,10\\}$ represent the cluster numbers. Each cluster is a multivariate bernoulli random variable and hence $$\\begin{equation} p(x | z = k) = \\prod_{j = 1}^d \\theta_{kj}^{x_j} (1 - \\theta_{kj})^{1 - x_j}  \\\\ ∀ k ∈ \\{1,2,...,10\\} \\end{equation} $$ where $p(z = k) = \\pi_k$\n",
    "Using this we write $$p(x,z) = p(x | z = k) p(z = k) = \\prod_{k = 1}^{10} (\\pi_k p(x | \\theta_k))^{z_k} $$ where $z_k = 1$ if the data point belongs to cluster k. This is the one hot encoding trick discussed in class. Now given a dataset $D = \\{x_1, x_2, x_3, ...., x_N\\}$, the likelihood can be written as $$p(X, Z) = \\prod_{i = 1}^N p(x_i, z) = \\prod_{i = 1}^N \\prod_{k = 1}^{10} (\\pi_k p(x_i | \\theta_k))^{z_{ik}}$$ This is the total data likelihood that we want to maximize in the presence of unknown/latent variables. Taking the log of this, we have $$log(p(X,Z)) = \\sum_{i = 1}^N \\sum_{k = 1}^{10} z_{ik} log(p(x_i | z = k) = \\sum_{i = 1}^N \\sum_{k = 1}^{10} z_{ik} (log(\\pi_k) + \\sum_{j = 1}^d (x_{ij}log(\\theta_{kj}) + (1 - x_{ij})log(1 - \\theta_{kj})))$$ This is the function that we would like to maximise and get the optimal values of $\\theta$ and $\\pi$. But since the latent variables are unknow, what we do is we take the expectation of this expression wrt observed data $X$ and some initial guess for the parameters that we want to find $\\theta$ and $\\pi$. The only expression above that depends on the latent variable is $z_{ik}$ and since this is a one hot encoding variable, we know that $$E[z_{ik} | X; \\theta^o, \\pi^o] = P(z_{ik} = k | X; \\theta^o, \\pi^o)$$ Applying Bayes Theorem, we have $$P(z_{ik} = k | X; \\theta^o, \\pi^o) = \\frac{P(X | z_{ik} = k;  \\theta^o, \\pi^o) P(z_{ik} = k)}{\\sum_{k = 1}^{10} P(X | z_{ik} = k;  \\theta^o, \\pi^o) P(z_{ik} = k)}$$ from which we have $$ \\gamma_{ik} = \\frac{\\pi_k^o P(x_i | z = k; \\theta_k ^o)}{\\sum_{m = 1}^{10} \\pi_m^o P(x_i | z = m; \\theta_m ^ o)} $$ This is the E step of the EM algorithm. Now we have the expectation of the total data log likelihood as $$Q(\\theta, \\pi; \\theta^o, \\pi^o ) = E[log(p(X,Z))] = \\sum_{i = 1}^N \\sum_{k = 1}^{10} \\gamma_{ik} (log(\\pi_k) + \\sum_{j = 1}^d (x_{ij}log(\\theta_{kj}) + (1 - x_{ij})log(1 - \\theta_{kj})))$$ Now our aim in the M step is to find optimal parameters $\\theta$ and $\\pi$ that maximize the Q function $Q(\\theta, \\pi; \\theta^o, \\pi^o )$ subject to constraints $\\sum_{k = 1}^{10} \\pi_k = 1$ The result of the optimisation process is as follows \n",
    "$$\\pi_k^{o + 1} = \\frac{\\sum_{i = 1}^N \\gamma_{ik}}{N}$$ and $$\\theta_k^{o + 1} = \\frac{\\sum_{i = 1}^N \\gamma_{ik} x_{i}}{\\sum_{i = 1}^N \\gamma_{ik}}$$\n",
    "\n",
    "Thus, we have arrived at the end of M step. Now we repeat the process untill the change in parameters in negligible. So overall, the EM algorithm for Mixture of Bernoulli can be summarised as follows :- \n",
    "1. Start with a guess of parameters $\\pi^o$ and $\\theta^o$ where $\\sum_{k = 1}^{10} \\pi^o = 1$. \n",
    "2. E step : Compute $\\gamma_{ik}$ for $i \\in \\{1,2,...,N\\}$ and $k \\in \\{1,2,3...,10\\}$ \n",
    "$$\\gamma_{ik} = \\frac{\\pi_k^o P(x_i | z = k; \\theta_k ^o)}{\\sum_{m = 1}^{10} \\pi_m^o P(x_i | z = m; \\theta_m ^ o)} $$\n",
    "3. M step: Find the next iteration values of parameters $\\pi^{o + 1}$ and $\\theta^{o + 1}$ $$\\pi_k^{o + 1} = \\frac{\\sum_{i = 1}^N \\gamma_{ik}}{N}$$ $$\\theta_k^{o + 1} = \\frac{\\sum_{i = 1}^N \\gamma_{ik} x_{i}}{\\sum_{i = 1}^N \\gamma_{ik}}$$\n",
    "\n",
    "Using this framework, lets try to build a mixture model for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d290a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_EM_MOB(x , pi, theta, K = 10): \n",
    "    '''\n",
    "    :input arguments ---> data set, mixture probabilities, initial cluster locations and number of mixtures\n",
    "    : Implement the EM algorithm for Mixture of Bernoulli random variables\n",
    "    : HINT -> We strongly recommend to avoid two nested for loops. Try vectorising your code. \n",
    "    : A simple example of vectorisation is as follows :- \n",
    "    : Assume you have two vectors a and b. Suppose you want to find out the dot product of a and b. \n",
    "    : There are two ways -> 1. sum = 0 ; for i in range(len(a)): sum += a[i]*b[i] \n",
    "    : 2. sum = a.T @ b\n",
    "    : The second method uses the matrix properties to get the same result as first. This is the main idea. \n",
    "    : Also it is a good idea to try out your own code with a small minibatch of data instead of the full data. \n",
    "    '''\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814a4f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_set\n",
    "#Initialise the P(z = k) = $$\\pi_k$$\n",
    "pi = \n",
    "#Initialise theta\n",
    "theta =\n",
    "#First try out with a small dataset\n",
    "pi_new, theta_new = train_EM_MOB(train[:500] , pi , theta)\n",
    "#After ensuring it works as expected, run on full dataset\n",
    "pi_new, theta_new = train_EM_MOB(train , pi, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd86c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise the 10 clusters that you obtained\n",
    "get_cluster_plot(theta_new) #Get the cluster representation for a mixture model with 10 mixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fc5191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of COGS118B WI22 Project1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
